1.分布式系统是由一组通过网络进行通信、为了完成共同的任务而协调工作的计算机节点组成的系统。分布式系统的出现是为了用廉价的、普通的机器完成单个计算机无法完成的计算
、存储任务。其目的是利用更多的机器，处理更多的数据。
2.那么分布式系统怎么将任务分发到这些计算机节点呢，很简单的思想，分而治之，即分片（partition）。对于计算，那么就是对计算任务进行切换，每个节点算一些，最终汇总就
行了，这就是MapReduce的思想；对于存储，更好理解一下，每个节点存一部分数据就行了。当数据规模变大的时候，Partition是唯一的选择，同时也会带来一些好处：
（1）提升性能和并发，操作被分发到不同的分片，相互独立
（2）提升系统的可用性，即使部分分片不能用，其他分片不会受到影响
3.理想的情况下，有分片就行了，但事实的情况却不大理想。原因在于，分布式系统中有大量的节点，且通过网络通信。单个节点的故障（进程crash、断电、磁盘损坏）是个小概
率事件，但整个系统的故障率会随节点的增加而指数级增加，网络通信也可能出现断网、高延迟的情况。在这种一定会出现的“异常”情况下，分布式系统还是需要继续稳定的对外提
供服务，即需要较强的容错性。最简单的办法，就是冗余或者复制集（Replication），即多个节点负责同一个任务，最为常见的就是分布式存储中，多个节点复杂存储同一份数据，
以此增强可用性与可靠性。同时，Replication也会带来性能的提升，比如数据的locality可以减少用户的等待时间。
4.分布式系统的挑战
1）第一，异构的机器与网络：
    分布式系统中的机器，配置不一样，其上运行的服务也可能由不同的语言、架构实现，因此处理能力也不一样；节点间通过网络连接，而不同网络运营商提供的网络的带宽、
	延时、丢包率又不一样。怎么保证大家齐头并进，共同完成目标，这四个不小的挑战。
2）第二，普遍的节点故障：
    虽然单个节点的故障概率较低，但节点数目达到一定规模，出故障的概率就变高了。分布式系统需要保证故障发生的时候，系统仍然是可用的，这就需要监控节点的状态，
	在节点故障的情况下将该节点负责的计算、存储任务转移到其他节点
3）第三，不可靠的网络：
    节点间通过网络通信，而网络是不可靠的。可能的网络问题包括：网络分割、延时、丢包、乱序。相比单机过程调用，网络通信最让人头疼的是超时：节点A向节点B发出请求，
	在约定的时间内没有收到节点B的响应，那么B是否处理了请求，这个是不确定的，这个不确定会带来诸多问题，最简单的，是否要重试请求，节点B会不会多次处理同一个请求。
5.分布式系统特性与衡量标准：
1）透明性：使用分布式系统的用户并不关心系统是怎么实现的，也不关心读到的数据来自哪个节点，对用户而言，分布式系统的最高境界是用户根本感知不到这是一个分布式系统。
2）可扩展性：分布式系统的根本目标就是为了处理单个计算机无法处理的任务，当任务增加的时候，分布式系统的处理能力需要随之增加。简单来说，要比较方便的通过增加机器
             来应对数据量的增长，同时，当任务规模缩减的时候，可以撤掉一些多余的机器，达到动态伸缩的效果。
3）可用性与可靠性：一般来说，分布式系统是需要长时间甚至7*24小时提供服务的。可用性是指系统在各种情况对外提供服务的能力，简单来说，可以通过不可用时间与正常服务
                   时间的必知来衡量；而可靠性而是指计算结果正确、存储的数据不丢失。
4）高性能：不管是单机还是分布式系统，大家都非常关注性能。不同的系统对性能的衡量指标是不同的，最常见的：高并发，单位时间内处理的任务越多越好；低延迟：每个任务
           的平均时间越少越好。这个其实跟操作系统CPU的调度策略很像
5）一致性：分布式系统为了提高可用性可靠性，一般会引入冗余（复制集）。那么如何保证这些节点上的状态一致，这就是分布式系统不得不面对的一致性问题。一致性有很多等
           级，一致性越强，对用户越友好，但会制约系统的可用性；一致性等级越低，用户就需要兼容数据不一致的情况，但系统的可用性、并发性很高很多。
6.性质分解
1）可扩展性（SCALABILITY）
第一：分片分式，即按照什么算法对任务进行拆分
常见的算法包括：哈希（hash），一致性哈希（consistency hash），基于数据范围（range based）。每一种算法有各自的优缺点，也就有各自的适用场景。
第二：分片的键，partition key	
第三：分片的额外好处
　　提升性能和并发：不同的请求分发到不同的分片
    提高可用性：一个分片挂了不影响其他的分片
第四：分片带来的问题
　　如果一个操作需要跨越多个分片，那么效率就会很低下，比如数据中的join操作
第五：元数据管理
　　元数据记录了分片与节点的映射关系、节点状态等核心信息，分布式系统中，有专门的节点（节点集群）来管理元数据，我们称之为元数据服务器。元数据服务器有以下特点：
　　高性能：cache
　　高可用：冗余 加 快速failover
　　强一致性（同时只有一个节点对外提供服务）
第六：任务的动态均衡
　　为了达到动态均衡，需要进行数据的迁移，如何保证在迁移的过程中保持对外提供服务，这也是一个需要精心设计的复杂问题。
2）可用性
i.可用性（Availability）是系统不间断对外提供服务的能力，可用性是一个度的问题，最高目标就是7 * 24，即永远在线。
ii.冗余是提高可用性、可靠性的法宝。冗余就是说多个节点负责相同的任务，在需要状态维护的场景，比如分布式存储中使用非常广泛。
iii.中心化就是有一个主节点（primary master）负责调度数据的更新，其优点是协议简单，将并发操作转变为顺序操作，缺点是primar可能成为瓶颈，且在primary故障的
    时候重新选举会有一段时间的不可用。
iV.去中心化就是所有节点地位平等，都能够发起数据的更新，优点是高可用，缺点是协议复杂，要保证一致性很难。	
3）一致性
i.一致性与可用性在分布式系统中的关系，已经有足够的研究，形成了CAP理论。CAP理论就是说分布式数据存储，最多只能同时满足一致性（C，Consistency）、
可用性（A， Availability）、分区容错性（P，Partition Tolerance）中的两者。但一致性和可用性都是一个度的问题，是0到1，而不是只有0和1两个极端。
ii.一致性从系统的角度和用户的角度有不同的等级。
    系统角度的一致性：强一致性、若一致性、最终一致性
    用户角度的一致性：单调读一致性，单调写一致性，读后写一致性，写后读一致性
4）高性能
i.衡量指标：高并发、高吞吐、低延迟、不同的系统关注的核心指标不一样，比如MapReduce，本身就是离线计算，无需低延迟
ii.可行的办法：单个节点的scaleup、分片（partition）、缓存：比如元数据、短事务
7.CAP理论介绍
1）CAP理论是说对于分布式数据存储，最多只能同时满足一致性（C，Consistency）、可用性（A， Availability）、分区容错性（P，Partition Tolerance）中的两者。
　　一致性，是指对于每一次读操作，要么都能够读到最新写入的数据，要么错误。
　　可用性，是指对于每一次请求，都能够得到一个及时的、非错的响应，但是不保证请求的结果是基于最新写入的数据。
　　分区容错性，是指由于节点之间的网络问题，即使一些消息对包或者延迟，整个系统能继续提供服务（提供一致性或者可用性）。
8.数据分片
	                  映射难度	               元数据	                                  节点增删	                                                        数据动态均衡
hash方式	           简单         非常简单，几乎不用修改	                            需要迁移的数据比较多	                                                不支持
consistent hash
without virtual node   简单	        比较简单，取决于节点规模，几乎不用修改	    增删节点的时候只影响hash环上相邻节点，但不能使所有节点都参与数据迁移过程	    不支持
consistent hash 
with virtual node	   中等	     稍微复杂一些，主要取决于虚拟节点规模，很少修改	       需要迁移的数据比较少，且所有节点都能贡献部分数据	              若支持（修改虚拟节点与物理节点映射关系）
range based	          较为复杂	取决于每个块的大小，一般来说规模较大；且修改频率较高	需要迁移的数据比较少，且所有节点都能贡献部分数据	                    支持，且比较容易

1）分片特征值的选取：基于最常用的访问模式。访问时包括对数据的增删改查的
2）怎么保证多个备份的数据一致性？
   多个副本的一致性、可用性是CAP理论讨论的范畴，这里简单介绍两种方案。第一种是主从同步，首先选出主服务器，只有主服务器提供对外服务，主服务器将元数据的变革
   信息以日志的方式持久化到共享存储（例如nfs），然后从服务器从共享存储读取日志并应用，达到与主服务器一致的状态，如果主服务器被检测到故障（比如通过心跳），
   那么会重新选出新的主服务器。第二种方式，通过分布式一致性协议来达到多个副本件的一致，比如大名鼎鼎的Paxos协议，以及工程中使用较多的Paxos的特化版本 -- Raft
   协议，协议可以实现所有备份均可以提供对外服务，并且保证强一致性。
3）元数据的缓存
   即使元数据服务器可以由一组物理机器组成，也保证了副本集之间的一致性问题。但是如果每次对数据的请求都经过元数据服务器的话，元数据服务器的压力也是非常大的。
   很多应用场景，元数据的变化并不是很频繁，因此可以在访问节点上做缓存，这样应用可以直接利用缓存数据进行数据读写，减轻元数据服务器压力。
i.怎么达到缓存的强一致性呢？比较容易想到的办法是当metadata变化的时候立即通知所有的缓存服务器（mongos），但问题是通信有延时，不可靠。
   解决不一致的问题，一个比较常见的思路是版本号，比如网络通信，通信协议可能会发生变化，通信双方为了达成一致，那么可以使用版本号。在缓存一致性的问题上，也
可以使用版本号，基本思路是请求的时候带上缓存的版本号，路由到具体节点之后比较实际数据的版本号，如果版本号不一致，那么表示缓存信息过旧，此时需要从元数据服务
器重新拉取元数据并缓存。在MongoDB中，mongos缓存上就是使用的这种办法。另外一种解决办法，就是大名鼎鼎的lease机制 
9.lease机制
Lease机制提出的时候是为了解决分布式存储系统中缓存一致性的问题，那么首先来看看Lease机制是怎么保证缓存的强一致性的。
1）服务器向所有缓存发送缓存数据的同时，颁发一个lease，lease包含一个有限期（即过期时间）
2）lease的含义是：在这个有效期内，服务器保证元数据不会发生变化
3）因此缓存在这个有效期内可以放心大胆的使用缓存的元数据，如果超过了有效期，就不能使用数据了，就得去服务器请求。
4）如果外部请求修改服务器上的元数据（元数据的修改一定在服务器上进行），那么服务器会阻塞修改请求，直到所有已颁发的lease过期，然后修改元数据，并将新的元数据和
新的lease发送到缓存
5）如果元数据没有发生变化，那么服务器也需要在之前已颁发的lease到期之间，重新给缓存颁发新的lease（只有lease，没有数据）
10.中心化复制集
所谓的中心化，就是对于副本集的更新操作有一个中心节点来协调管理，将分布式的并发操作转化为单点的并发操作，从而保证副本集内各节点的一致性。其优点在于逻辑简单，
将复杂的问题（分布式并发）转换成一个有成熟解决方案的问题（单点并发）。但缺点在于，副本集的可用性依赖于中心节点，如果中心节点故障，即使有中心节点自动切换机制，
也会出现数10秒的不可用。（mangoDB）
1）主从节点数据更新流程:复制集之间数据的同步是同步模式还是异步模式,取决于系统对一致性、可用性、响应延迟的要求。
i.同步（Synchronous replication），就是说对于客户端请求，系统阻塞到复制集中所有节点都更新完成，才能向客户端返回，即write all。而异步（Asynchronous replication）模式，
只要一个或者部分节点更新则算写入操作成功，通常是write one。
ii.在同步模式下，系统的可靠性非常好，只要有一个节点正常，就能保证数据不丢失。但是系统的更新可用性非常差，只要有一个节点异常，就无法完成更新；而且，响应延迟比较大，
取决于副本集中网络延时最大、处理速度最慢的节点。
iii.　异步模式下，系统的吞吐量会比较好，但是存在数据丢失的风险，比如上图中，如果在数据同步到s2 s3之前s1挂掉，那么刚才客户端的更新就丢失了，关键在于客户端认为已经写
入成功了。另外，异步模式下，客户端在写入成功之后，立刻从系统读取数据，有可能读不到最新的数据，比如上图中，客户端写入s1之后立刻从s2 读取。
2）数据流向：链式与主从模式
i.链式就是指从一个节点推送到最近的节点，写入过程中每个节点的带宽利用都比较均衡，可以充分利用网络资源，也不会有单点压力；但是需要经过多个节点，写入延迟会比较大。
ii.而主从模式则是指数据同时从primary节点到secondary节点
MongoDB也是采用的链式模式;
3）部分节点写入失败
某个节点上数据部分写入（写入部分数据就崩溃了）；由于重试是对复制集中所有节点重试，因此某个节点上同一份数据可能写入了多份。对于第一个问题，由于有checksum，因此不怕
部分写入失败；第二个问题，由于有offset和元数据，重复写入也不是问题。haystack（以及GFS）通过这种巧妙的方式解决了分布式更新问题。
4）主从节点数据读取
复制集中，不同的系统在数据读取方面有两个问题。第一：secondary节点是否提供读服务；第二，如果可以从Secondary读取，那么这个接口是否开放给用户
i.第一个问题，如果secondary节点提供数据读取服务，那么是否会读取到过期的数据（即不是最新成功写入的数据） ？比如在异步写入的时候，客户端得到成功写入的返回之后，立即
	去secondary上读取，那么有可能读到过时的数据，这对于强一致性的情况是不能允许的。我们知道，元数据的管理一般也是复制集，而元数据需要保证强一致性，因此，元数据的写入
	一般都是同步的。比如GFS中，master由一个active（也就是primary节点）、多个standby（也就是secondary节点）组成，在元数据写入到active的时候，要保证本地和远程机器都写
	入成功才能返回；而且只有active提供读取服务。
ii.第二个问题，如果复制集中的节点都能提供读取服务，那么接口是否提供给最终用户呢？在haystack中，多个在不同机器上的物理卷组成一个逻辑卷，一个逻辑卷就是一个复制集。当
	读取请求到达的时候，是由haystack的元数据服务器（directory ）根据负载均衡的原则选出提供服务的物理卷，即用户是不知道读取请求是落地到哪个物理节点的。而对于mongodb，
	用户可以在查询语句里面指定是从Primary读取，还是从Secondary读取，或者让系统来选择（Nearest）。	
5）主节点选举
类似于redis的选举

全文参考网址：https://www.cnblogs.com/xybaby/p/7787034.html

解决分布式一致性问题，三种主要的协议与算法：二阶段提交协议、三阶段提交协议和Paxos算法(类似于redis中的选举制度)

分布式事物：
    分布式事务是指会涉及到操作多个数据库的事务。其实就是将对同一库事务的概念扩大到了对多个库的事务。目的是为了保证分布式系统中的数据一致性。分布式事务处理的关键是必须
有一种方法可以知道事务在任何地方所做的所有动作，提交或回滚事务的决定必须产生统一的结果（全部提交或全部回滚）。
    在分布式系统中，各个节点之间在物理上相互独立，通过网络进行沟通和协调。由于存在事务机制，可以保证每个独立节点上的数据操作可以满足ACID。但是，相互独立的节点之间无法
准确的知道其他节点中的事务执行情况。所以从理论上讲，两台机器理论上无法达到一致的状态。如果想让分布式部署的多台机器中的数据保持一致性，那么就要保证在所有节点的数据写操作，
要不全部都执行，要么全部的都不执行。但是，一台机器在执行本地事务的时候无法知道其他机器中的本地事务的执行结果。所以他也就不知道本次事务到底应该commit还是 roolback。所以，
常规的解决办法就是引入一个“协调者”的组件来统一调度所有分布式节点的执行。

XA规范
X/Open 组织（即现在的 Open Group ）定义了分布式事务处理模型。 X/Open DTP 模型（ 1994 ）包括应用程序（ AP ）、事务管理器（ TM ）、资源管理器（ RM ）、通信资源管理器（ CRM ）四部分。
一般，常见的事务管理器（ TM ）是交易中间件，常见的资源管理器（ RM ）是数据库，常见的通信资源管理器（ CRM ）是消息中间件。    通常把一个数据库内部的事务处理，如对多个表的操作，作为
本地事务看待。数据库的事务处理对象是本地事务，而分布式事务处理的对象是全局事务。   所谓全局事务，是指分布式事务处理环境中，多个数据库可能需要共同完成一个工作，这个工作即是一个全局
事务，例如，一个事务中可能更新几个不同的数据库。对数据库的操作发生在系统的各处但必须全部被提交或回滚。此时一个数据库对自己内部所做操作的提交不仅依赖本身操作是否成功，还要依赖与全局
事务相关的其它数据库的操作是否成功，如果任一数据库的任一操作失败，则参与此事务的所有数据库所做的所有操作都必须回滚。     一般情况下，某一数据库无法知道其它数据库在做什么，因此，
在一个 DTP 环境中，交易中间件是必需的，由它通知和协调相关数据库的提交或回滚。而一个数据库只将其自己所做的操作（可恢复）影射到全局事务中。    

XA 就是 X/Open DTP 定义的交易中间件与数据库之间的接口规范（即接口函数），交易中间件用它来通知数据库事务的开始、结束以及提交、回滚等。 XA 接口函数由数据库厂商提供。 
	
2PC
二阶段提交(Two-phaseCommit)是指，在计算机网络以及数据库领域内，为了使基于分布式系统架构下的所有节点在进行事务提交时保持一致性而设计的一种算法(Algorithm)。通常，二阶段提交也被称为
是一种协议(Protocol))。在分布式系统中，每个节点虽然可以知晓自己的操作时成功或者失败，却无法知道其他节点的操作的成功或失败。当一个事务跨越多个节点时，为了保持事务的ACID特性，需要引
入一个作为协调者的组件来统一掌控所有节点(称作参与者)的操作结果并最终指示这些节点是否要把操作结果进行真正的提交(比如将更新后的数据写入磁盘等等)。因此，二阶段提交的算法思路可以概括
为：参与者将操作成败通知协调者，再由协调者根据所有参与者的反馈情报决定各参与者是否要提交操作还是中止操作。

所谓的两个阶段是指：第一阶段：准备阶段(投票阶段)和第二阶段：提交阶段（执行阶段）。	
 
准备阶段：
    事务协调者(事务管理器)给每个参与者(资源管理器)发送Prepare消息，每个参与者要么直接返回失败(如权限验证失败)，要么在本地执行事务，写本地的redo和undo日志，但不提交，到达一种“万事俱备
，只欠东风”的状态。
	
可以进一步将准备阶段分为以下三个步骤：
1）协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。
2）参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作）
3）各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。	

提交阶段
    如果协调者收到了参与者的失败消息或者超时，直接给每个参与者发送回滚(Rollback)消息；否则，发送提交(Commit)消息；参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理
过程中使用的锁资源。(注意:必须在最后阶段释放锁资源)	

接下来分两种情况分别讨论提交阶段的过程。
当协调者节点从所有参与者节点获得的相应消息都为”同意”时:	
1）协调者节点向所有参与者节点发出”正式提交(commit)”的请求。
2）参与者节点正式完成操作，并释放在整个事务期间内占用的资源。
3）参与者节点向协调者节点发送”完成”消息。
4）协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。	

如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时：
1）协调者节点向所有参与者节点发出”回滚操作(rollback)”的请求。
2）参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。
3）参与者节点向协调者节点发送”回滚完成”消息。
4）协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务。
	
不管最后结果如何，第二阶段都会结束当前事务。

二阶段提交看起来确实能够提供原子性的操作，但是不幸的事，二阶段提交还是有几个缺点的：
1、同步阻塞问题。执行过程中，所有参与节点都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。
2、单点故障。由于协调者的重要性，一旦协调者发生故障。参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中，而无法继续完成事务操作。
（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题）
3、数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，这回导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据部一致性的现象。
4、二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事
务是否被已经提交。

 由于二阶段提交存在着诸如同步阻塞、单点问题、脑裂等缺陷，所以，研究者们在二阶段提交的基础上做了改进，提出了三阶段提交。	
3PC
三阶段提交（Three-phase commit），也叫三阶段提交协议（Three-phase commit protocol），是二阶段提交（2PC）的改进版本。	
也就是说，除了引入超时机制之外，3PC把2PC的准备阶段再次一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。

CanCommit阶段
3PC的CanCommit阶段其实和2PC的准备阶段很像。协调者向参与者发送commit请求，参与者如果可以提交就返回Yes响应，否则返回No响应。
1.事务询问 协调者向参与者发送CanCommit请求。询问是否可以执行事务提交操作。然后开始等待参与者的响应。
2.响应反馈 参与者接到CanCommit请求之后，正常情况下，如果其自身认为可以顺利执行事务，则返回Yes响应，并进入预备状态。否则反馈No

PreCommit阶段
协调者根据参与者的反应情况来决定是否可以记性事务的PreCommit操作。根据响应情况，有以下两种可能。
假如协调者从所有的参与者获得的反馈都是Yes响应，那么就会执行事务的预执行。
1.发送预提交请求 协调者向参与者发送PreCommit请求，并进入Prepared阶段。
2.事务预提交 参与者接收到PreCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。
3.响应反馈 如果参与者成功的执行了事务操作，则返回ACK响应，同时开始等待最终指令。
假如有任何一个参与者向协调者发送了No响应，或者等待超时之后，协调者都没有接到参与者的响应，那么就执行事务的中断。
1.发送中断请求 协调者向所有参与者发送abort请求。
2.中断事务 参与者收到来自协调者的abort请求之后（或超时之后，仍未收到协调者的请求），执行事务的中断。

doCommit阶段
该阶段进行真正的事务提交，也可以分为以下两种情况。

执行提交
1.发送提交请求 协调接收到参与者发送的ACK响应，那么他将从预提交状态进入到提交状态。并向所有参与者发送doCommit请求。
2.事务提交 参与者接收到doCommit请求之后，执行正式的事务提交。并在完成事务提交之后释放所有事务资源。
3.响应反馈 事务提交完之后，向协调者发送Ack响应。
4.完成事务 协调者接收到所有参与者的ack响应之后，完成事务。

中断事务 协调者没有接收到参与者发送的ACK响应（可能是接受者发送的不是ACK响应，也可能响应超时），那么就会执行中断事务。
1.发送中断请求 协调者向所有参与者发送abort请求
2.事务回滚 参与者接收到abort请求之后，利用其在阶段二记录的undo信息来执行事务的回滚操作，并在完成回滚之后释放所有的事务资源。
3.反馈结果 参与者完成事务回滚之后，向协调者发送ACK消息
4.中断事务 协调者接收到参与者反馈的ACK消息之后，执行事务的中断。	

相对于2PC，3PC主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会
导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之
间存在数据不一致的情况。	
	
https://blog.csdn.net/huaweitman/article/details/50758907

RAFT算法
https://www.jdon.com/artichect/raft.html	

OceanBase 的特性：高性能、低成本、高可用、可扩展、兼容性
OceanBase三种连接算法：Hash join、Merge join、Nestted Loop join
Nested Loop Join就是扫描一个表（外表），每读到该表中的一条记录，就去'扫描'另一张表（内表）找到满足条件的数据。这里的'扫描'可以是利用索引快速定位扫描，也可以是全表扫描。
所以Oceanbase支持对内表进行一次扫描并把结果物化在内存中，这样的话下次就可以直接在内存中扫描相关的数据，而不需要从存储层进行多次扫描。但是物化在内存中是有代价的，所以
Oceanbase的优化器基于代价去判断是否需要物化内表。

Merge Join首先会按照连接的字段对两个表进行sort(如果内存空间不够，就需要进行外排)，然后开始扫描两张表进行Merge。Merge的过程会从每个表取一条记录开始匹配，如果符合关联条件，
则放入结果集中；否则，将关联字段值较小的记录抛弃，从这条记录对应的表中取下一条记录继续进行匹配，直到整个循环结束。在多对多的两张表上进行Merge时，通常需要使用临时空间进行操作。

Hash Join就是用两个表中相对较小的表(通常称为build table)根据连接条件创建hash table，然后逐行扫描较大的表（通常称为prob table）并通过探测hash table找到匹配的行。 如果build table
非常大，构建的hash table无法在内存中容纳时，Oceanbase会分别将build table和probe table按照连接条件切分成多个分区（partition），每个partition都包括一个独立的、成对匹配的build table
和probe table，这样就将一个大的hash join切分成多个独立、互相不影响的hash join，每一个分区的hash join都能够在内存中完成。
	
分区：
range分区：范围、连续
list分区：集合、不要求连续、离散
hash分区：主要用来确保数据在预先确定数目的分区中平均分布，你所要做的只是基于将要被哈希的列值指定一个列值或表达式，以及指定被分区的表将要被分割成的分区数量。
key分区:跟hash分区类似，也是通过对分区个数取模的方式来确定数据属于哪个分区。不同的是系统会对key分区键做一个内部默认的hash函数后再取模。
分区优点：
1，分区可以分在多个磁盘，存储更大一点
2，根据查找条件，也就是where后面的条件，查找只查找相应的分区不用全部查找了
3，进行大数据搜索时可以进行并行处理。
4，跨多个磁盘来分散数据查询，来获得更大的查询吞吐量	

https://blog.csdn.net/u013613428/article/details/55259924	
CAP，BASE和最终一致性是NoSQL数据库存在的三大基石。而五分钟法则是内存数据存储的理论依据。这个是一切的源头。	
C:Consistency 一致性、A: Availability 可用性(指的是快速获取数据)、P: Tolerance of network Partition 分区容忍性(分布式)
一个分布式系统不可能满足一致性，可用性和分区容错性这三个需求，最多只能同时满足两个。
　  1）一致性，是指对于每一次读操作，要么都能够读到最新写入的数据，要么错误。
　　2）可用性，是指对于每一次请求，都能够得到一个及时的、非错的响应，但是不保证请求的结果是基于最新写入的数据。
　　3）分区容错性，是指由于节点之间的网络问题，即使一些消息对包或者延迟，整个系统能继续提供服务（提供一致性或者可用性）。	
CA：传统关系数据库（单机数据库）
AP：key-value数据库（部分NoSQL）	
	
最终一致性：过程松，结果紧，最终结果必须保持一致性
强一致性
强一致性（即时一致性） 假如A先写入了一个值到存储系统，存储系统保证后续A,B,C的读取操作都将返回最新值

弱一致性
假如A先写入了一个值到存储系统，存储系统不能保证后续A,B,C的读取操作能读取到最新值。此种情况下有一个“不一致性窗口”的概念，它特指从A写入值，到后续操作A,B,C读取到最新值这一段时间。

最终一致性
最终一致性是弱一致性的一种特例。假如A首先write了一个值到存储系统，存储系统保证如果在A,B,C后续读取之前没有其它写操作更新同样的值的话，最终所有的读取操作都会读取到最A写入的最新值。
此种情况下，如果没有失败发生的话，“不一致性窗口”的大小依赖于以下的几个因素：交互延迟，系统的负载，以及复制技术中replica的个数（这个可以理解为master/salve模式中，salve的个数），
最终一致性方面最出名的系统可以说是DNS系统，当更新一个域名的IP以后，根据配置策略以及缓存控制策略的不同，最终所有的客户都会看到最新的值。
	
BASE模型反ACID模型，完全不同ACID模型，牺牲高一致性，获得可用性或可靠性： Basically Available基本可用。支持分区失败(e.g. sharding碎片划分数据库) Soft state软状态 状态可以有一段时
间不同步，异步。 Eventually consistent最终一致，最终数据是一致的就可以了，而不是时时一致。 
BASE思想的主要实现有 
1.按功能划分数据库 
2.sharding碎片  
BASE思想主要强调基本的可用性，如果你需要高可用性，也就是纯粹的高性能，那么就要以一致性或容错性为牺牲，BASE思想的方案在性能上还是有潜力可挖的。 
BASE是Basically Available（基本可用）、Soft state（软状态）和Eventually consistent（最终一致性）三个短语的简写，BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统
分布式实践的结论，是基于CAP定理逐步演化而来的，其核心思想是即使无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性
（Eventual consistency）。	

分布式系统是一个非常广泛的概念，它最终要落实到解决实际问题上，不同的问题有不同的方法和架构。所有的开源软件都是以某个应用场景出现，而纯粹以“分布式”概念进行划分的比较少见。
但如果以算法划分，到能分出几类：
1.以Leader选举为主的一类算法，比如paxos、viewstamp，就是现在zookeeper、Chuby等工具的主体
2.以分布式事务为主的一类主要是二段提交，这些分布式数据库管理器及数据库都支持
3.以若一致性为主的，主要代表是Cassandra的W、R、N可调节的一致性
4.以租赁机制为主的，主要是一些分布式锁的概念，目前还没有看到纯粹“分布式”锁的实现
5.以失败探测为主的，主要是Gossip和phi失败探测算法，当然也包括简单的心跳
6.以弱一致性、因果一致性、顺序一致性为主的，开源尚不多，但大都应用在Linkedin、Twitter、Facebook等公司内部
7当然以异步解耦为主的，还有各类Queue
	
zookeeper：ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，它是集群的管理者，监视着集群中各个节点的状态根据节点提交的反馈进行下一步
合理操作。最终，将简单易用的接口和性能高效、功能稳定的系统提供给用户。

2.ZooKeeper提供了
1)文件系统
2)通知机制

3.Zookeeper文件系统
每个子目录项如 NameService 都被称作为znode，和文件系统一样，我们能够自由的增加、删除znode，在一个znode下增加、删除子znode，唯一的不同在于znode是可以存储数据的。 

所谓集群管理无在乎两点：是否有机器退出和加入、选举master。 
对于第一点，所有机器约定在父目录GroupMembers下创建临时目录节点，然后监听父目录节点的子节点变化消息。一旦有机器挂掉，该机器与 zookeeper的连接断开，其所创建的临时目录节点被删除，
所有其他机器都收到通知：某个兄弟目录被删除，于是，所有人都知道：它上船了。
新机器加入也是类似，所有机器收到通知：新兄弟目录加入，highcount又有了，对于第二点，我们稍微改变一下，所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为master就好。

4.Zookeeper通知机制

客户端注册监听它关心的目录节点，当目录节点发生变化（数据改变、被删除、子目录节点增加删除）时，zookeeper会通知客户端。

5.Zookeeper做了什么？

1.命名服务   2.配置管理   3.集群管理   4.分布式锁  5.队列管理

6.Zookeeper命名服务

在zookeeper的文件系统里创建一个目录，即有唯一的path。在我们使用tborg无法确定上游程序的部署机器时即可与下游程序约定好path，通过path即能互相探索发现。

7.Zookeeper的配置管理

程序总是需要配置的，如果程序分散部署在多台机器上，要逐个改变配置就变得困难。现在把这些配置全部放到zookeeper上去，保存在 Zookeeper 的某个目录节点中，然后所有相关应用程序对这个目录节
点进行监听，一旦配置信息发生变化，每个应用程序就会收到 Zookeeper 的通知，然后从 Zookeeper 获取新的配置信息应用到系统中就好

8.Zookeeper集群管理

所谓集群管理无在乎两点：是否有机器退出和加入、选举master。 
对于第一点，所有机器约定在父目录GroupMembers下创建临时目录节点，然后监听父目录节点的子节点变化消息。一旦有机器挂掉，该机器与 zookeeper的连接断开，其所创建的临时目录节点被删除，所有其他机器都收到通知：某个兄弟目录被删除，于是，所有人都知道：它上船了。
新机器加入也是类似，所有机器收到通知：新兄弟目录加入，highcount又有了，对于第二点，我们稍微改变一下，所有机器创建临时顺序编号目录节点，每次选取编号最小的机器作为master就好。

9.Zookeeper分布式锁

有了zookeeper的一致性文件系统，锁的问题变得容易。锁服务可以分为两类，一个是保持独占，另一个是控制时序。 
对于第一类，我们将zookeeper上的一个znode看作是一把锁，通过createznode的方式来实现。所有客户端都去创建 /distribute_lock 节点，最终成功创建的那个客户端也即拥有了这把锁。
用完删除掉自己创建的distribute_lock 节点就释放出锁。 
对于第二类， /distribute_lock 已经预先存在，所有客户端在它下面创建临时顺序编号目录节点，和选master一样，编号最小的获得锁，用完删除，依次方便。

10.Zookeeper队列管理
两种类型的队列：
1、同步队列，当一个队列的成员都聚齐时，这个队列才可用，否则一直等待所有成员到达。 
2、队列按照 FIFO 方式进行入队和出队操作。 
第一类，在约定目录下创建临时目录节点，监听节点数目是否是我们要求的数目。 
第二类，和分布式锁服务中的控制时序场景基本原理一致，入列有编号，出列按编号。
11.分布式与数据复制 
Zookeeper作为一个集群提供一致的数据服务，自然，它要在所有机器间做数据复制。数据复制的好处： 
1、容错：一个节点出错，不致于让整个系统停止工作，别的节点可以接管它的工作； 
2、提高系统的扩展能力 ：把负载分布到多个节点上，或者增加节点来提高系统的负载能力； 
3、提高性能：让客户端本地访问就近的节点，提高用户访问速度。 
从客户端读写访问的透明度来看，数据复制集群系统分下面两种： 
1、写主(WriteMaster) ：对数据的修改提交给指定的节点。读无此限制，可以读取任何一个节点。这种情况下客户端需要对读与写进行区别，俗称读写分离； 
2、写任意(Write Any)：对数据的修改可提交给任意的节点，跟读一样。这种情况下，客户端对集群节点的角色与变化透明。
对zookeeper来说，它采用的方式是写任意。通过增加机器，它的读吞吐能力和响应能力扩展性非常好，而写，随着机器的增多吞吐能力肯定下降（这也是它建立observer的原因），而响应能力则取决于具体实现方式，是延迟复制保持最终一致性，还是立即复制快速响应。

11.角色
    领导者（leader）：领导者负责进行投票的发起和决议，更新系统状态
    跟随者（follower）：用于接收客户请求并向客户端返回结果，在选主工程中参与投票
    观察者（observer）：observer可以接收客户端连接，将写请求转发给leader节点。但observer不参加投票过程，只同步leader的状态。observer的目的是为了扩展系统，提高读取速度
    客户端（client）：请求发起方
14.Zookeeper设计目的
1.最终一致性：client不论连接到哪个Server，展示给它都是同一个视图，这是zookeeper最重要的性能。 
2.可靠性：具有简单、健壮、良好的性能，如果消息被到一台服务器接受，那么它将被所有的服务器接受。 
3.实时性：Zookeeper保证客户端将在一个时间间隔范围内获得服务器的更新信息，或者服务器失效的信息。但由于网络延时等原因，Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果
需要最新数据，应该在读数据之前调用sync()接口。 
4.等待无关（wait-free）：慢的或者失效的client不得干预快速的client的请求，使得每个client都能有效的等待。 
5.原子性：更新只能成功或者失败，没有中间状态。 
6.顺序性：包括全局有序和偏序两种：全局有序是指如果在一台服务器上消息a在消息b前发布，则在所有Server上消息a都将在消息b前被发布；偏序是指如果一个消息b在消息a后被同一个发送者发布，a必将排在b前面。 

15.Zookeeper工作原理
Zookeeper 的核心是原子广播，这个机制保证了各个Server之间的同步。实现这个机制的协议叫做Zab协议。Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者
在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和 leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。 
为了保证事务的顺序一致性，zookeeper采用了递增的事务id号（zxid）来标识事务。所有的提议（proposal）都在被提出的时候加上了zxid。实现中zxid是一个64位的数字，它高32位是epoch用来标
识leader关系是否改变，每次一个leader被选出来，它都会有一个新的epoch，标识当前属于那个leader的统治时期。低32位用于递增计数。

16.Zookeeper 下 Server工作状态
每个Server在工作过程中有三种状态： 
LOOKING：当前Server不知道leader是谁，正在搜寻
LEADING：当前Server即为选举出来的leader
FOLLOWING：leader已经选举出来，当前Server与之同步	

17.Zookeeper选主流程(basic paxos)
当leader崩溃或者leader失去大多数的follower，这时候zk进入恢复模式，恢复模式需要重新选举出一个新的leader，让所有的Server都恢复到一个正确的状态。Zk的选举算法有两种：一种是基
于basic paxos实现的，另外一种是基于fast paxos算法实现的。系统默认的选举算法为fast paxos。
1.选举线程由当前Server发起选举的线程担任，其主要功能是对投票结果进行统计，并选出推荐的Server； 
2.选举线程首先向所有Server发起一次询问(包括自己)； 
3.选举线程收到回复后，验证是否是自己发起的询问(验证zxid是否一致)，然后获取对方的id(myid)，并存储到当前询问对象列表中，最后获取对方提议的leader相关信息(id,zxid)，并将这些信息存储到
当次选举的投票记录表中； 
4.收到所有Server回复以后，就计算出zxid最大的那个Server，并将这个Server相关信息设置成下一次要投票的Server； 
5.线程将当前zxid最大的Server设置为当前Server要推荐的Leader，如果此时获胜的Server获得n/2 + 1的Server票数，设置当前推荐的leader为获胜的Server，将根据获胜的Server相关信息设置自己的状
态，否则，继续这个过程，直到leader被选举出来。 通过流程分析我们可以得出：要使Leader获得多数Server的支持，则Server总数必须是奇数2n+1，且存活的Server的数目不得少于n+1. 每个Server启动
后都会重复以上流程。在恢复模式下，如果是刚从崩溃状态恢复的或者刚启动的server还会从磁盘快照中恢复数据和会话信息，zk会记录事务日志并定期进行快照，方便在恢复时进行状态恢复。


20.Zookeeper工作流程-Leader

1 .恢复数据； 
2 .维持与Learner的心跳，接收Learner请求并判断Learner的请求消息类型； 
3 .Learner的消息类型主要有PING消息、REQUEST消息、ACK消息、REVALIDATE消息，根据不同的消息类型，进行不同的处理。 
PING 消息是指Learner的心跳信息；
REQUEST消息是Follower发送的提议信息，包括写请求及同步请求；
ACK消息是 Follower的对提议的回复，超过半数的Follower通过，则commit该提议；
REVALIDATE消息是用来延长SESSION有效时间。

21.Zookeeper工作流程-Follower
Follower主要有四个功能： 
1.向Leader发送请求（PING消息、REQUEST消息、ACK消息、REVALIDATE消息）； 
2.接收Leader消息并进行处理； 
3.接收Client的请求，如果为写请求，发送给Leader进行投票；
4.返回Client结果。 

Follower的消息循环处理如下几种来自Leader的消息： 
1 .PING消息： 心跳消息； 
2 .PROPOSAL消息：Leader发起的提案，要求Follower投票； 
3 .COMMIT消息：服务器端最新一次提案的信息； 
4 .UPTODATE消息：表明同步完成； 
5 .REVALIDATE消息：根据Leader的REVALIDATE结果，关闭待revalidate的session还是允许其接受消息； 
6 .SYNC消息：返回SYNC结果到客户端，这个消息最初由客户端发起，用来强制得到最新的更新。


redis、分布式中的Gossip协议也有“病毒感染算法”、“谣言传播算法”之称
在一个有界网络中，每个节点都随机地与其他节点通信，经过一番杂乱无章的通信，最终所有节点的状态都会达成一致。每个节点可能知道所有其他节点，也可能仅知道几个邻居节点，只要这些节
可以通过网络连通，最终他们的状态都是一致的，当然这也是疫情传播的特点。


分布式系统的Raft算法
　　过去, Paxos一直是分布式协议的标准，但是Paxos难于理解，更难以实现，Google的分布式锁系统Chubby作为Paxos实现曾经遭遇到很多坑。
Paxos和Raft的区别在于选举的具体过程不同。
　　在Raft中，任何时候一个服务器可以扮演下面角色之一：
Leader: 处理所有客户端交互，日志复制等，一般一次只有一个Leader.
Follower: 类似选民，完全被动
Candidate候选人: 类似Proposer律师，可以被选为一个新的领导人。
Raft阶段分为两个，首先是选举过程，然后在选举出来的领导人带领进行正常操作，比如日志复制等。下面用图示展示这个过程：
1. 任何一个服务器都可以成为一个候选者Candidate，它向其他服务器Follower发出要求选举自己的请求：
2. 其他服务器同意了，发出OK。
注意如果在这个过程中，有一个Follower当机，没有收到请求选举的要求，因此候选者可以自己选自己，只要达到N/2 + 1 的大多数票，候选人还是可以成为Leader的。
3. 这样这个候选者就成为了Leader领导人，它可以向选民也就是Follower们发出指令，比如进行日志复制。
4. 以后通过心跳进行日志复制的通知
5. 如果一旦这个Leader当机崩溃了，那么Follower中有一个成为候选者，发出邀票选举。
6. Follower同意后，其成为Leader，继续承担日志复制等指导工作：

幂等性：就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。


























	